# -*- coding: utf-8 -*-
"""Commented Working code - ConvHAN glove

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l_ARJ7MHh3DJXTWmQD0XEHXmRN_RHq7c
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
# from tqdm import tqdm


import numpy as np
import pandas as pd
import pickle
from collections import defaultdict
import re
import sys
import os
from keras.preprocessing.text import Tokenizer, text_to_word_sequence
from keras.preprocessing.sequence import pad_sequences
from keras.utils.np_utils import to_categorical

from keras.layers import Embedding
from keras.layers import Dense, Input, Flatten
from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional, TimeDistributed
from keras.models import Model

from keras import backend as K
from keras.engine.topology import Layer, InputSpec
from keras import initializers
#from keras.layers import Merge
#replaced by
from keras.layers import add

# NLTK import, download
import nltk
from nltk import tokenize # used only for splitting data into words, sentences
nltk.download(['punkt', 'stopwords'], quiet=True)

from matplotlib import pyplot as plt

#import my modules
from Clean_Texts import clean_text
from AttentionLayer import AttLayer

def read_data(filename):
    df = pd.read_csv(filename,engine='python', delimiter=r'\t+', names=['id', 'Label','Statement', 'subject', 'speaker','speaker_title','state', 'party','barely_true','false_counts','half_true','mostly_true','pants_on_fire','context'])
    labels=[]
    for l in df['Label']:
        if l in ['false','barely-true','pants-fire']:
            labels.append(0)
        elif l in ['half-true','mostly-true', 'true']:
            labels.append(1)
        else:
            raise Exception("Error encountered in labelling")
    data = pd.concat([df['Statement'], pd.Series(labels)], axis=1)
    data.columns = ['Statement', 'Label']
    return data

data_train = read_data('LIAR_dataset/train.tsv')
data_test = read_data('LIAR_dataset/test.tsv')
# data_valid = read_data('valid.tsv') # not using currently valid tsv

train_labels = np.asarray(data_train['Label'])
test_labels = np.asarray(data_test['Label'])

#Just checking whether it doesn't contain any garbage
# data_train[data_train.Statement.str.contains("json")]

y_tr = [len(train_labels) - sum(train_labels), sum(train_labels)]
x_tr = ["Fake", "Not Fake"]
plt.figure(figsize=(8,8))
plt.bar(x_tr,y_tr)
for i in range(2):
    plt.text(x_tr[i], y_tr[i]+60, str(y_tr[i]),ha='center', color='blue', fontsize=18)
plt.ylim(top=6200)
plt.xlabel('Class')
plt.ylabel('No. of samples')
plt.title('Training class distribution')
plt.rcParams.update({'font.size': 21})
plt.savefig('plots/train_class_dist.png')

y_tr = [len(test_labels) - sum(test_labels), sum(test_labels)]
x_tr = ["Fake", "Not Fake"]
plt.figure(figsize=(8,8))
plt.bar(x_tr,y_tr)
for i in range(2):
    plt.text(x_tr[i], y_tr[i]+30, str(y_tr[i]),ha='center', color='blue', fontsize=18)
plt.ylim(top=800)
plt.xlabel('Class')
plt.ylabel('No. of samples')
plt.title('Test class distribution')
plt.rcParams.update({'font.size': 18})
plt.savefig('plots/test_class_dist.png')

"""**TODO: Classes are imbalanced, need to be taken care of**"""

# ! pip install transformers
# import transformers
# tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
# bert_model = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased')

print(data_train.Statement[1])
print(clean_text(data_train.Statement[1]))
print(clean_text('He was dead'))
# Reason for not cleaning as it'll lose the meanings as shown below

print(tokenize.sent_tokenize(data_train.Statement[1]))
#Split sample article into sentences array -> NLTK tokenizer

def process_text(df,clean=False):
    reviews = [] # contains the array of sentences for each sample
    labels = [] # contains label for each sample 
    texts = [] # contains the raw text article for each sample
    for idx in range(df.Statement.shape[0]):
        text = df.Statement[idx]
        if clean:
            text = clean_text(text) # clean text (regex + stemming)
        texts.append(text)
        sentences = tokenize.sent_tokenize(text) # split articles into sentences
        reviews.append(sentences)
        labels.append(data_train.Label[idx])
    return reviews, labels, texts

reviews, labels, texts = process_text(data_train, clean=False)
reviews_test, labels_test, texts_test = process_text(data_test, clean=False)
print('reviews: contains the array of sentences for each sample, \nreviews[1]:')
print(reviews[1])

def get_max_params(reviews):
    max_sent_article = 1
    max_word_article = 1
    for article in reviews:
        if len(article)>max_sent_article:
            max_sent_article = len(article)
        for sentence in article:
            words = sentence.split()
            if len(words)>max_word_article:
                max_word_article = len(words)
    return max_sent_article, max_word_article

max_sent_article, max_word_article = get_max_params(reviews)
print('For train data:')
print('Max sentences in an article: ', max_sent_article)
print('Max words in a sentence', max_word_article)

max_sent_article, max_word_article = get_max_params(reviews_test)
print('For test data:')
print('Max sentences in an article: ', max_sent_article)
print('Max words in a sentence', max_word_article)

MAX_SENT_LENGTH = 54#maximum length of each sentence
MAX_SENTS = 11 #maximum number of sentences in one news article 
MAX_NB_WORDS = 400000 #no of words in vocabulary (glove)
EMBEDDING_DIM = 100 #glove embedding size
VALIDATION_SPLIT = 0.2

"""MAX_SENTS is 11 as max sentence value = 11

[Read this Stackoverflow thread to understand what does keras tokenizer method exacly do below?](https://stackoverflow.com/questions/51956000/what-does-keras-tokenizer-method-exactly-do)
"""

tokenizer = Tokenizer(num_words=MAX_NB_WORDS) # keras tokenizer
tokenizer.fit_on_texts(texts) # fit to update tokenizer's internal dictionary and their tokens
data = np.zeros((len(texts), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')
print('Input array shape:')
print(data.shape)

def fill_input_data(data, reviews, params, tokenizer):
    MAX_NB_WORDS = params[0]
    MAX_SENT_LENGTH = params[1]
    MAX_SENTS = params[2]
    for i, sentences in enumerate(reviews):
        for j, sent in enumerate(sentences):
            if j < MAX_SENTS:
                # choosing only sentences which is less than max num of sentences
                wordTokens = text_to_word_sequence(sent) #split sentences into words (keras)
                k = 0
                for _, word in enumerate(wordTokens):
                    if word not in tokenizer.word_index.keys():
                        continue
                    if k < MAX_SENT_LENGTH and tokenizer.word_index[word] < MAX_NB_WORDS:
                        # for each sentence check if no. of words<max sentence length by iterating over each word
                        # and also check the total no. of unique tokens doesn't exceed MAX_NB_words ie vocab of glove
                        data[i, j, k] = tokenizer.word_index[word]
                        k = k + 1
            else:
                raise Exception('Too many sentences in an article')

params = [MAX_NB_WORDS, MAX_SENT_LENGTH, MAX_SENTS]
fill_input_data(data, reviews, params, tokenizer)
word_index = tokenizer.word_index
print('Total %s unique tokens.' % len(word_index)) # total no. of words in vocabulary (tokenized) fitted on articles

labels = np.asarray(labels)
print('Shape of data tensor:', data.shape)
print('Shape of label tensor:', labels.shape)

data_test = np.zeros((len(texts_test), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')
fill_input_data(data_test, reviews_test, params, tokenizer)
labels_test = np.asarray(labels_test)

print(len(texts)) #Total data samples on which tokenizer is fitted

# Split train data into train and valid; shuffled
indices = np.arange(data.shape[0])
np.random.shuffle(indices)
data = data[indices]
labels = labels[indices]
nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])

x_train = data[:-nb_validation_samples]
y_train = labels[:-nb_validation_samples]
x_val = data[-nb_validation_samples:]
y_val = labels[-nb_validation_samples:]

x_test=data_test
y_test=labels_test
print('Number of true and fake news in training and validation set')
print(y_train.sum(axis=0), len(y_train)-y_train.sum(axis=0))
print(y_val.sum(axis=0), len(y_val)-y_val.sum(axis=0))

"""#Generating embedding_index dict"""

embeddings_index = {}
f = open('glove/glove.6B.100d.txt',encoding='utf-8')
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()

"""## Example Explanation for above code
line = "the -0.234 0.14 0.41 -0.12 0.12 ..."

word = "the"

coefs = "-0.234 0.14 0.41..."

embedding_index is a dict where key is word and value is list of 100 dimensional vector coefficients for EMDEDDING_DIM=100
"""

print("A sample embedding vector for 'the'")
print(embeddings_index['the'])

print('Total %s word vectors i.e. the length of embedding_index dict = MAX_NB_WORDS' % len(embeddings_index))

embedding_matrix = np.random.random((len(word_index)+1, EMBEDDING_DIM))
#keras required the first arg to be our words vocab(word_index) length (not glove vocab) + 1
for word, i in word_index.items():
    try:
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector
    except KeyError:
        print('Word: %s not found in glove vocabulary', word)

#keras required the first arg to be our words vocab (word_index) length (not glove vocab) + 1
embedding_layer = Embedding(len(word_index)+1,
                            EMBEDDING_DIM,
                            weights=[embedding_matrix],
                            input_length=MAX_SENT_LENGTH,
                            trainable=False)
print('Embedding Matrix is of dimensions: ', embedding_matrix.shape)

word_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')
embedded_sequences = embedding_layer(word_input)

c=Conv1D(filters=128,kernel_size=3,padding='valid',activation='relu')(embedded_sequences)
maxPool=MaxPooling1D(pool_size=2)(c)

#customized f1 score function for passing into model metrics:
def F1_Score(y_true, y_pred): #taken from old keras source code
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    recall = true_positives / (possible_positives + K.epsilon())
    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())
    return f1_val

l_lstm = Bidirectional(GRU(100, return_sequences=True))(maxPool)
l_att = AttLayer(100)(l_lstm)
wordEncoder = Model(word_input, l_att)

sentence_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')
sentence_encoder = TimeDistributed(wordEncoder)(sentence_input)

c2=Conv1D(filters=128,kernel_size=3,padding='valid',activation='relu')(sentence_encoder)
maxPool2=MaxPooling1D(pool_size=2)(c2)

l_lstm_sent = Bidirectional(GRU(100, return_sequences=True))(maxPool2)
l_att_sent = AttLayer(100)(l_lstm_sent)
preds = Dense(1, activation='sigmoid')(l_att_sent)
model_Att = Model(sentence_input, preds)
model_Att.summary()
from keras.utils.vis_utils import plot_model
plot_model(model_Att, to_file='model_plot.png', show_shapes=True, show_layer_names=True)
model_Att.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy',F1_Score,])
history = model_Att.fit(x_train, y_train, validation_data=(x_val, y_val),
          epochs=50, batch_size=32)

score=model_Att.evaluate(x_test,y_test,verbose=1)
print('Test Accuracy: '+str(score[1]))
print('Test F1 Score: '+str(score[2]))

from sklearn.metrics import precision_recall_fscore_support,classification_report
y_pred=model_Att.predict(x_test)
#print(y_pred)
y2=[]
for q in y_pred:
  if(q[0]>0.5):
    y2.append(True)
  else:
    y2.append(False)
print('Classification report:\n',classification_report(y_test,y2))
#print('Classification report:\n',precision_recall_fscore_support(y_test,y_pred))
#print(y_pred)

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y2)
print('Confusion Matrix')
print(cm)

# list all data in history
print(history.history.keys())
# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'valid'], loc='upper left')
# plt.show()
plt.savefig('plots/Model_accuracy.png')

# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'valid'], loc='upper left')
# plt.show()
plt.savefig('plots/Model_loss.png')