# -*- coding: utf-8 -*-
"""Copy of Working code - ConvHAN glove

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l_ARJ7MHh3DJXTWmQD0XEHXmRN_RHq7c
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x
from tqdm import tqdm

!wget https://www.cs.ucsb.edu/~william/data/liar_dataset.zip

!unzip 'liar_dataset.zip'

import numpy as np
import pandas as pd
import pickle
from collections import defaultdict
import re

from bs4 import BeautifulSoup

import sys
import os


from keras.preprocessing.text import Tokenizer, text_to_word_sequence
from keras.preprocessing.sequence import pad_sequences
from keras.utils.np_utils import to_categorical

from keras.layers import Embedding
from keras.layers import Dense, Input, Flatten
from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional, TimeDistributed
from keras.models import Model

from keras import backend as K
from keras.engine.topology import Layer, InputSpec
from keras import initializers

#from keras.layers import Merge
#replaced by
from keras.layers import add

!wget https://raw.githubusercontent.com/Tawkat/Fake-News-Detection/master/Codes/Neural_Network_based_Methods/DataPrep/Clean_Texts.py

from Clean_Texts import clean_text

!wget https://raw.githubusercontent.com/Tawkat/Fake-News-Detection/master/Codes/Neural_Network_based_Methods/AttentionLayer/AttentionLayer.py

from AttentionLayer import AttLayer

import nltk
nltk.download('punkt')


# data_train = pd.read_csv('train.tsv', delimiter='\t',names=['id', 'Label','Statement', 'subject', 'speaker','speaker_title','state', 'party','barely_true','false_counts','half_true','mostly_true','pants_on_fire','context'])
# print(data_train.shape)

# data_test = pd.read_csv('test.tsv', delimiter='\t',names=['id', 'Label','Statement', 'subject', 'speaker','speaker_title','state', 'party','barely_true','false_counts','half_true','mostly_true','pants_on_fire','context'])
# print(data_test.shape)
train_df = pd.read_csv('train.tsv', delimiter='\t', names=['id', 'Label','Statement', 'subject', 'speaker','speaker_title','state', 'party','barely_true','false_counts','half_true','mostly_true','pants_on_fire','context'])
train_labels=[]
for l in train_df['Label']:
    if l in ['false','barely-true','pants-fire']:
        train_labels.append(0)
    elif l in ['half-true','mostly-true', 'true']:
        train_labels.append(1)
    else:
        print("Error encountered in labelling")
train_data = pd.concat([train_df['Statement'], pd.Series(train_labels)], axis=1)
train_data.columns = ['Statement', 'Label']

data_train = train_data

test_df = pd.read_csv('test.tsv', delimiter='\t', names=['id', 'label','statement', 'subject', 'speaker','speaker_title','state', 'party','barely_true','false_counts','half_true','mostly_true','pants_on_fire','context'])
test_labels=[]
for l in test_df['label']:
    if l in ['false','barely-true','pants-fire']:
        test_labels.append(0)
    elif l in ['half-true','mostly-true', 'true']:
        test_labels.append(1)
    else:
        print("Error encountered in labelling")
test_data = pd.concat([test_df['statement'], pd.Series(test_labels)], axis=1)
test_data.columns = ['Statement', 'Label']
data_test = test_data

valid_df = pd.read_csv('valid.tsv', delimiter='\t', names=['id', 'label','statement', 'subject', 'speaker','speaker_title','state', 'party','barely_true','false_counts','half_true','mostly_true','pants_on_fire','context'])
valid_labels=[]
for l in valid_df['label']:
    if l in ['false','barely-true','pants-fire']:
        valid_labels.append(0)
    elif l in ['half-true','mostly-true', 'true']:
        valid_labels.append(1)
    else:
        print("Error encountered in labelling")
valid_data = pd.concat([valid_df['statement'], pd.Series(valid_labels)], axis=1)
valid_data.columns = ['Statement', 'Label']
data_valid = valid_data

from nltk import tokenize
nltk.download('stopwords')

MAX_SENT_LENGTH = 100 #maximum sentence length
MAX_SENTS = 20 #maximum number of sentences in one news article
MAX_NB_WORDS = 400000 #no of words in vocalubary (glove)
EMBEDDING_DIM = 100 #glove embedding size
VALIDATION_SPLIT = 0.2

data_train

print(data_train.Statement[0])
# print(clean_text(data_train.Statement[0]))
tokenize.sent_tokenize()

reviews = []
labels = []
texts = []
for idx in range(data_train.Statement.shape[0]):
    text = data_train.Statement[idx]
    # text = clean_text(text) # clean text (regex + stemming)
    texts.append(text)
    sentences = tokenize.sent_tokenize(text) # split articles into sentences
    reviews.append(sentences)
    labels.append(data_train.Label[idx])

tokenizer = Tokenizer(num_words=MAX_NB_WORDS)
tokenizer.fit_on_texts(texts) # fit to update tokenizer's internal dictionary and their tokens
# https://stackoverflow.com/questions/51956000/what-does-keras-tokenizer-method-exactly-do
data = np.zeros((len(texts), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')



for i, sentences in enumerate(reviews):
    for j, sent in enumerate(sentences):
        if j < MAX_SENTS:
            #choosing only sentences which is less than max num of sentences
            wordTokens = text_to_word_sequence(sent) #split sentences into words
            k = 0
            for _, word in enumerate(wordTokens):
                if k < MAX_SENT_LENGTH and tokenizer.word_index[word] < MAX_NB_WORDS:
                    data[i, j, k] = tokenizer.word_index[word]
                    k = k + 1

word_index = tokenizer.word_index
print('Total %s unique tokens.' % len(word_index))

labels =(np.asarray(labels))
print('Shape of data tensor:', data.shape)
print('Shape of label tensor:', labels.shape)

reviews_test = []
labels_test = []
texts_test = []

for idx in range(data_test.Statement.shape[0]):
    text = data_test.Statement[idx]
    # text = clean_text(text)
    texts_test.append(text)
    sentences = tokenize.sent_tokenize(text)
    reviews_test.append(sentences)

    labels_test.append(data_test.Label[idx])

data_test = np.zeros((len(texts_test), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')

for i, sentences in enumerate(reviews_test):
    for j, sent in enumerate(sentences):
        if j < MAX_SENTS:
            wordTokens = text_to_word_sequence(sent)
            k = 0
            for _, word in enumerate(wordTokens):
                if word not in tokenizer.word_index.keys():
                    continue
                if k < MAX_SENT_LENGTH and tokenizer.word_index[word] < MAX_NB_WORDS:
                    data_test[i, j, k] = tokenizer.word_index[word]
                    k = k + 1

labels_test = np.asarray(labels_test)

reviews_valid = []
labels_valid = []
texts_valid = []

for idx in range(data_valid.Statement.shape[0]):
    text = data_valid.Statement[idx]
    text = clean_text(text)
    texts_valid.append(text)
    sentences = tokenize.sent_tokenize(text)
    reviews_valid.append(sentences)

    labels_valid.append(data_valid.Label[idx])

data_valid = np.zeros((len(texts_valid), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')

for i, sentences in enumerate(reviews_valid):
    for j, sent in enumerate(sentences):
        if j < MAX_SENTS:
            wordTokens = text_to_word_sequence(sent)
            k = 0
            for _, word in enumerate(wordTokens):
                if word not in tokenizer.word_index.keys():
                    continue
                if k < MAX_SENT_LENGTH and tokenizer.word_index[word] < MAX_NB_WORDS:
                    data_valid[i, j, k] = tokenizer.word_index[word]
                    k = k + 1

labels_valid = np.asarray(labels_valid)

indices = np.arange(data.shape[0])
np.random.shuffle(indices)
data = data[indices]
labels = labels[indices]
nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])

x_train = data[:-nb_validation_samples]
y_train = labels[:-nb_validation_samples]
x_val = data[-nb_validation_samples:]
y_val = labels[-nb_validation_samples:]

x_test=data_test
y_test=labels_test

print('Number of positive and negative News in training and validation set')
print(y_train.sum(axis=0))
print(y_val.sum(axis=0))



!wget http://nlp.stanford.edu/data/glove.6B.zip

!unzip glove*.zip

embeddings_index = {}
f = open('glove.6B.100d.txt',encoding='utf-8')
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()

print('Total %s word vectors.' % len(embeddings_index))

embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector


embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector



embedding_layer = Embedding(len(word_index) + 1,
                            EMBEDDING_DIM,
                            weights=[embedding_matrix],
                            input_length=MAX_SENT_LENGTH,
                            trainable=False)



word_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')
embedded_sequences = embedding_layer(word_input)

c=Conv1D(filters=128,kernel_size=3,padding='valid',activation='relu')(embedded_sequences)
maxPool=MaxPooling1D(pool_size=2)(c)

l_lstm = Bidirectional(GRU(100, return_sequences=True))(maxPool)
l_att = AttLayer(100)(l_lstm)
wordEncoder = Model(word_input, l_att)

sentence_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')
sentence_encoder = TimeDistributed(wordEncoder)(sentence_input)

c2=Conv1D(filters=128,kernel_size=3,padding='valid',activation='relu')(sentence_encoder)
maxPool2=MaxPooling1D(pool_size=2)(c2)

l_lstm_sent = Bidirectional(GRU(100, return_sequences=True))(maxPool2)
l_att_sent = AttLayer(100)(l_lstm_sent)

preds = Dense(1, activation='sigmoid')(l_att_sent)

model_Att = Model(sentence_input, preds)
model_Att.summary()
'''from keras.utils.vis_utils import plot_model
plot_model(model_Att, to_file='Merged/model_plot.png', show_shapes=True, show_layer_names=True)'''
model_Att.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['acc'])
model_Att.fit(x_train, y_train, validation_data=(x_val, y_val),
          epochs=10, batch_size=128)



score=model_Att.evaluate(x_test,y_test,verbose=1)
print('acc: '+str(score[1]))

from sklearn.metrics import precision_recall_fscore_support,classification_report
y_pred=model_Att.predict(x_test)
#print(y_pred)
y2=[]
for q in y_pred:
  if(q[0]>0.5):
    y2.append(True)
  else:
    y2.append(False)
print('Classification report:\n',classification_report(y_test,y2))
#print('Classification report:\n',precision_recall_fscore_support(y_test,y_pred))
#print(y_pred)

